{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e1b3d0-4d24-4f1e-b57f-57f3fcb7cf1b",
   "metadata": {},
   "source": [
    "# Documentation for bot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae825ba9-ffbd-4730-994b-9e49e2b6ae09",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "- `import os`  : Import the os module for operating system functionality\n",
    "\n",
    "- `from dotenv import load_dotenv` : Import load_dotenv to load environment variables from a `.env` file\n",
    "\n",
    "- `from langchain.chains.combine_documents import create_stuff_documents_chain` : Import function for combining documents\n",
    "\n",
    "- `from langchain.schema import Document` : Import Document schema from langchain\n",
    "\n",
    "- `from langchain_core.prompts import PromptTemplate` : Import prompt template for generating prompts\n",
    "\n",
    "- `from langchain_mistralai.chat_models import ChatMistralAI` : Import ChatMistralAI model for conversation\n",
    "\n",
    "- `from langchain_milvus import Milvus` : Import Milvus for vector storage\n",
    "\n",
    "- `from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader` : Import loaders for web documents\n",
    "\n",
    "- `from langchain_text_splitters import RecursiveCharacterTextSplitter` : Import splitter for text documents\n",
    "\n",
    "- `from langchain.chains import create_retrieval_chain` : Import function to create a retrieval chain\n",
    "\n",
    "- `from langchain_huggingface import HuggingFaceEmbeddings` : Import HuggingFace embeddings\n",
    "\n",
    "- `from pymilvus import connections, utility` : Import Milvus connection and utility functions\n",
    "\n",
    "- `from requests.exceptions import HTTPError`  : Import HTTPError for handling HTTP exceptions\n",
    "\n",
    "- `from httpx import HTTPStatusError` : Import HTTPStatusError for HTTP status exceptions\n",
    "\n",
    "- Load environment variables from `.env` file\n",
    "\n",
    "`load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")  # Get MISTRAL API key from environment variables`\n",
    "\n",
    "- Set USER_AGENT environment variable if not already set\n",
    "\n",
    "`if not os.getenv(\"USER_AGENT\"): \n",
    "    os.environ[\"USER_AGENT\"] = \"my_custom_user_agent\"  \n",
    "\n",
    "- `MILVUS_URI = \"./milvus/milvus_vector.db\"` : Define the URI for the Milvus database\n",
    "\n",
    "- `MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"` : Specify the model name for embeddings\n",
    "\n",
    "- `CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'` : Define the source of documents\n",
    "\n",
    "- Print statement summarizing the purpose of the code\n",
    "    `print(\"Initializes environment, loads documents, sets embeddings.\")`  # Output a summary of initialization actions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37046ed-6dcf-4f02-97e6-c40cfacb85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializes environment, loads documents, sets embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain.schema import Document\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "#from langchain_mistralai import MistralAIEmbeddings\n",
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "#from langchain_cohere import ChatCohere\n",
    "from langchain_milvus import Milvus\n",
    "from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from pymilvus import connections, utility\n",
    "from requests.exceptions import HTTPError\n",
    "from httpx import HTTPStatusError\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Set USER_AGENT environment variable if not already set\n",
    "if not os.getenv(\"USER_AGENT\"):\n",
    "    os.environ[\"USER_AGENT\"] = \"my_custom_user_agent\"  # You can customize this string\n",
    "\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'\n",
    "\n",
    "print(\"Initializes environment, loads documents, sets embeddings.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d8f9-f26a-4641-aaa0-9c97e0194fd6",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "\n",
    "- `import os` - Import the os module for operating system functionality\n",
    "\n",
    "- `os.environ['TQDM_DISABLE'] = '1' ` : Suppress tqdm warnings by disabling its output\n",
    "\n",
    "- `from langchain_huggingface import HuggingFaceEmbeddings` : Import HuggingFaceEmbeddings for embedding generation\n",
    "\n",
    "- `MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"` : Specify the model name for embeddings\n",
    "\n",
    "- Create an embedding function using the specified model\n",
    "\n",
    "   `def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model.\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)`\n",
    "    \n",
    "- Output a summary of the function's purpose\n",
    "    print(\"Returns Hugging Face model embedding function.\")  \n",
    "\n",
    "- Return the created embedding function\n",
    "    return embedding_function \n",
    "\n",
    "- Call the function to trigger the print statement, get the embedding function and store the returned embedding function in a variable\n",
    "`embedding_function = get_embedding_function()  \n",
    "print(f\"Embedding function created with model: {MODEL_NAME}\")`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3f8267-7fce-4155-a0de-a9c38014adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns Hugging Face model embedding function.\n",
      "Embedding function created with model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['TQDM_DISABLE'] = '1'  # Suppress tqdm warnings\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model.\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)\n",
    "    \n",
    "    print(\"Returns Hugging Face model embedding function.\")\n",
    "    \n",
    "    return embedding_function\n",
    "\n",
    "# Call the function to trigger the print statement and get the embedding function\n",
    "embedding_function = get_embedding_function()\n",
    "\n",
    "print(f\"Embedding function created with model: {MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e6ef4-cf3e-4b46-807d-f168f750a83f",
   "metadata": {},
   "source": [
    "- `from langchain_mistralai.chat_models import ChatMistralAI` : Import ChatMistralAI for generating responses\n",
    "\n",
    "- `from httpx import HTTPStatusError` : Import HTTPStatusError for handling HTTP errors\n",
    "\n",
    "- `MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"` : Define the model name for embeddings\n",
    "\n",
    "- `MILVUS_URI = \"./milvus/milvus_vector.db\"` : Specify the URI for the local Milvus database\n",
    "\n",
    "- Placeholder function for creating a prompt\n",
    "`def create_prompt():`\n",
    "\n",
    "- Return a prompt template\n",
    "    `return \"Provide a detailed summary of the latest research on: {input}\"` \n",
    "\n",
    "- Placeholder function for loading the vector store\n",
    "   `def load_exisiting_db(uri):`\n",
    "    \n",
    "- Return itself for retrieval\n",
    "\n",
    "   `class VectorStore:\n",
    "        def as_retriever(self):\n",
    "            return self`\n",
    "\n",
    "- Return an instance of the VectorStore class\n",
    "    return VectorStore() \n",
    "\n",
    "- Placeholder function for creating a document chain\n",
    "   `def create_stuff_documents_chain(model, prompt):`\n",
    "\n",
    "- Return a placeholder for the document chain\n",
    "    `return \"Document chain here.\"`\n",
    "\n",
    "- Placeholder function for creating a retrieval chain\n",
    "    `def create_retrieval_chain(retriever, document_chain):`\n",
    "\n",
    "- Return a lambda function simulating a retrieval process\n",
    "    `return lambda x: { \n",
    "        \"context\": [  # Example context with metadata sources\n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper1\"}},\n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper2\"}}\n",
    "        ],`\n",
    "        \"answer\": \"Generated response\"  # Placeholder answer\n",
    "    }\n",
    "\n",
    " def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store, \n",
    "    \n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    \n",
    "- Define the model\n",
    "    `model = ChatMistralAI(model='open-mistral-7b')`  : Instantiate the ChatMistralAI model\n",
    "\n",
    "    `print(\"Model Loaded\")` : Print confirmation of model loading\n",
    "    \n",
    "    `prompt = create_prompt()` : Create a prompt for the model\n",
    "\n",
    "- Load the vector store and create the retriever\n",
    "\n",
    "    `vector_store = load_exisiting_db(uri=MILVUS_URI)` : Load the existing vector store\n",
    "\n",
    "\n",
    "    `retriever = vector_store.as_retriever()`  : Get the retriever from the vector store\n",
    "    \n",
    "    try:\n",
    "\n",
    "  `document_chain = create_stuff_documents_chain(model, prompt)` : Create the document chain\n",
    "\n",
    "  `print(\"Document Chain Created\")` : Print confirmation of document chain creation\n",
    "\n",
    "  `retrieval_chain = create_retrieval_chain(retriever, document_chain)` : Create the retrieval chain\n",
    "\n",
    "  `print(\"Retrieval Chain Created\")` : Print confirmation of retrieval chain creation\n",
    "    \n",
    "- Generate a response to the query and invoke the retrieval chain with the query\n",
    "        response = retrieval_chain({\"input\": f\"{query}\"})\n",
    "\n",
    "    except HTTPStatusError as e:\n",
    "\n",
    "  `print(f\"HTTPStatusError: {e}\") `: Print any HTTP errors encountered\n",
    "\n",
    "  `if e.response.status_code == 429:` : Check for high traffic error\n",
    "\n",
    "   `return \"I am currently experiencing high traffic. Please try again later.\", []` : Return a message for high traffic\n",
    "\n",
    "   `return f\"HTTPStatusError: {e}\", []` : Return the error message if other error occurs\n",
    "    \n",
    "- Logic to add sources to the response\n",
    "\n",
    "   `max_relevant_sources = 4`  : Set the maximum number of sources to add to the response\n",
    "\n",
    "    `all_sources = \"\"` : Initialize string to hold all sources\n",
    "\n",
    "   `sources = []`  : Initialize list to hold unique sources\n",
    "\n",
    "    `count = 1 ` : Initialize a counter for source numbering\n",
    "    \n",
    "    `for i in range(max_relevant_sources):` : Loop through the maximum number of sources\n",
    "\n",
    "        `try:\n",
    "            source = response[\"context\"][i][\"metadata\"][\"source\"]`  :  Get the source from the response\n",
    "\n",
    "  - Check if the source is already added to the list\n",
    "    `if source not in sources:` : If source is not already in the list\n",
    "\n",
    "    `sources.append(source)`  : Add the source to the list\n",
    "    \n",
    "    `all_sources += f\"[Source {count}]({source}), \"` : Format the source for output\n",
    "\n",
    "    `count += 1` : Increment the source count\n",
    "  \n",
    "    `except IndexError:` : If there are no more sources to add\n",
    "\n",
    "    `break` : Exit the loop if an IndexError occurs\n",
    "    \n",
    "   `all_sources = all_sources[:-2]`  : Remove the last comma and space from the sources string\n",
    "\n",
    "    `response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"` : Append the sources to the response answer\n",
    "\n",
    "    ` print(\"Response Generated\")` : Print confirmation of response generation\n",
    "\n",
    "    `print(\"Initializes components, retrieves documents, generates response.\")` : Print summary of process\n",
    "\n",
    "    `return response[\"answer\"], sources`  : Return the generated answer and list of sources\n",
    "\n",
    "# Example usage\n",
    "- query = \"Latest research on machine learning in healthcare\"  : Define a sample query\n",
    "  \n",
    "- answer, sources = query_rag(query) : Call the query_rag function with the sample query\n",
    "\n",
    "- print(answer) : Print the generated answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9fa6c9-9fd5-412c-90df-9140cdbd942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Document Chain Created\n",
      "Retrieval Chain Created\n",
      "Response Generated\n",
      "Initializes components, retrieves documents, generates response.\n",
      "Generated response\n",
      "\n",
      "Sources: [Source 1](https://example.com/research_paper1), [Source 2](https://example.com/research_paper2)\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI\n",
    "from httpx import HTTPStatusError\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"\n",
    "\n",
    "def create_prompt():\n",
    "    # Placeholder function for creating a prompt\n",
    "    return \"Provide a detailed summary of the latest research on: {input}\"\n",
    "\n",
    "def load_exisiting_db(uri):\n",
    "    # Placeholder function for loading the vector store\n",
    "    class VectorStore:\n",
    "        def as_retriever(self):\n",
    "            return self\n",
    "    return VectorStore()\n",
    "\n",
    "def create_stuff_documents_chain(model, prompt):\n",
    "    # Placeholder function for creating a document chain\n",
    "    return \"Document chain here.\"\n",
    "\n",
    "def create_retrieval_chain(retriever, document_chain):\n",
    "    # Placeholder function for creating a retrieval chain\n",
    "    return lambda x: {\"context\": [{\"metadata\": {\"source\": \"https://example.com/research_paper1\"}}, {\"metadata\": {\"source\": \"https://example.com/research_paper2\"}}], \"answer\": \"Generated response\"}\n",
    "\n",
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store, \n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = ChatMistralAI(model='open-mistral-7b')\n",
    "    print(\"Model Loaded\")\n",
    "\n",
    "    prompt = create_prompt()\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_exisiting_db(uri=MILVUS_URI)\n",
    "    retriever = vector_store.as_retriever()\n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)\n",
    "        print(\"Document Chain Created\")\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)\n",
    "        print(\"Retrieval Chain Created\")\n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain({\"input\": f\"{query}\"})\n",
    "    except HTTPStatusError as e:\n",
    "        print(f\"HTTPStatusError: {e}\")\n",
    "        if e.response.status_code == 429:\n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []\n",
    "        return f\"HTTPStatusError: {e}\", [] \n",
    "    \n",
    "    # Logic to add sources to the response\n",
    "    max_relevant_sources = 4  # number of sources at most to be added to the response\n",
    "    all_sources = \"\"\n",
    "    sources = []\n",
    "    count = 1\n",
    "    for i in range(max_relevant_sources):\n",
    "        try:\n",
    "            source = response[\"context\"][i][\"metadata\"][\"source\"]\n",
    "            # Check if the source is already added to the list\n",
    "            if source not in sources:\n",
    "                sources.append(source)\n",
    "                all_sources += f\"[Source {count}]({source}), \"\n",
    "                count += 1\n",
    "        except IndexError:  # if there are no more sources to add\n",
    "            break\n",
    "    all_sources = all_sources[:-2]  # remove the last comma and space\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"\n",
    "    print(\"Response Generated\")\n",
    "\n",
    "    print(\"Initializes components, retrieves documents, generates response.\")\n",
    "\n",
    "    return response[\"answer\"], sources\n",
    "\n",
    "# Example usage\n",
    "query = \"Latest research on machine learning in healthcare\"\n",
    "answer, sources = query_rag(query)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220aed-32ea-4f03-9f56-9b36feb3f511",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- `create_vector_store` function is designed to manage a vector store using a Milvus database.\n",
    "\n",
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "    It connects to a local Milvus database specified by the URI. If a collection named \"research_paper_chatbot\" already exists,\n",
    "    it loads the existing vector store; otherwise, it creates a new vector store and drops any existing one.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "\n",
    "- Create the directory if it does not exist\n",
    "    `head = os.path.split(uri)` : Split the URI to get the directory path\n",
    "\n",
    "   `os.makedirs(head[0], exist_ok=True)`  : Create the directory if it doesn't exist\n",
    "\n",
    "    `print(\"Directory created for vector store if it did not exist\")` : Print confirmation of directory creation\n",
    "\n",
    "- Connect to the Milvus database\n",
    "\n",
    "    `connections.connect(\"default\", uri=uri)` : Establish a connection to the Milvus database\n",
    "\n",
    "    `print(\"Connected to the Milvus database\")` : Print confirmation of database connection\n",
    "\n",
    "- Check if the collection already exists\n",
    "\n",
    "- `if utility.has_collection(\"research_paper_chatbot\"):` - Check for existing collection\n",
    "\n",
    "- `print(\"Collection already exists. Loading existing Vector Store.\")` : Print collection status\n",
    "\n",
    "- `vector_store = Milvus` : Load existing vector store\n",
    "\n",
    "-  Get the embedding function\n",
    "\n",
    "  `collection_name=\"research_paper_chatbot\",\n",
    "            embedding_function=get_embedding_function()`\n",
    "            \n",
    "- `connection_args={\"uri\": uri}` - Connection parameters\n",
    "\n",
    "     `print(\"Existing Vector Store Loaded\")` : Print confirmation of loading existing store\n",
    "- else:\n",
    "  `vector_store = Milvus.from_documents` : Create a new vector store from documents\n",
    "\n",
    "  `documents=docs,` : Documents to store\n",
    "  \n",
    "   `embedding=embeddings,`  : Embedding function for documents\n",
    "\n",
    "   `collection_name=\"research_paper_chatbot\",` : Name of the collection\n",
    "\n",
    "   `connection_args={\"uri\": uri},` : Connection parameters\n",
    "  \n",
    "- `drop_old=True,` : Drop old collection if exists\n",
    "        )\n",
    "- `print(\"New Vector Store Created with provided documents\")` - Print confirmation of new store creation\n",
    "\n",
    "- `return vector_store` : Return the created or loaded vector store\n",
    "\n",
    "def load_exisiting_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "\n",
    "- `vector_store = Milvus(`  : Load the vector store\n",
    "\n",
    "- `collection_name=\"research_paper_chatbot\",` : Name of the collection\n",
    "\n",
    "- `embedding_function=get_embedding_function(),` : Get the embedding function\n",
    "\n",
    "- `connection_args={\"uri\": uri},` Connection parameters\n",
    "    )\n",
    "    \n",
    "- `print(\"Loaded existing Vector Store from Milvus database\")` : Print confirmation of store loading\n",
    "\n",
    "- `return vector_store` : Return the loaded vector store\n",
    "\n",
    "`if __name__ == '__main__':`:  Load documents from the web\n",
    "    \n",
    "- `print(\"Loading documents from the web...\")` : Print message before loading documents\n",
    "\n",
    "- `documents = load_documents_from_web()` : Load documents from the web\n",
    "\n",
    "- `print(f\"Loaded {len(documents)} documents from the web.\")` : Print number of documents loaded\n",
    "\n",
    "- Split the documents into chunks\n",
    "- `print(\"Splitting documents into chunks...\")` : Print message before splitting documents\n",
    "\n",
    "- `docs = split_documents(documents)` : Split loaded documents into chunk\n",
    "  \n",
    "- `print(f\"Split into {len(docs)} chunks.\")` : Print number of chunks created\n",
    "\n",
    "- Get the embedding function\n",
    "    `print(\"Getting embedding function...\")`\n",
    "      \n",
    "- `embeddings = get_embedding_function()`  : Retrieve the embedding function\n",
    "\n",
    "- Define the URI for the Milvus database Assign the URI for the Milvus database\n",
    "\n",
    "    `uri = MILVUS_URI`\n",
    "  \n",
    "- Call the functions to see print statements\n",
    "\n",
    "- `print(\"Creating vector store...\")` : Print message before creating vector store\n",
    "\n",
    "- `vector_store = create_vector_store(docs, embeddings, uri)` : Create the vector store\n",
    "\n",
    "- `print(\"Loading existing vector store...\")`  : Print message before loading existing store\n",
    "\n",
    "    `loaded_vector_store = load_exisiting_db(uri)`  : Load the existing vector store\n",
    "\n",
    "- `print(\"Finished operations.\")` : Print message indicating completion of operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fcdf03a-ad46-4b5b-9a6b-b27e970939b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from the web...\n",
      "Loaded 1 documents from the web.\n",
      "Splitting documents into chunks...\n",
      "Split into 6 chunks.\n",
      "Getting embedding function...\n",
      "Returns Hugging Face model embedding function.\n",
      "Creating vector store...\n",
      "Directory created for vector store if it did not exist\n",
      "Connected to the Milvus database\n",
      "Collection already exists. Loading existing Vector Store.\n",
      "Returns Hugging Face model embedding function.\n",
      "Existing Vector Store Loaded\n",
      "Loading existing vector store...\n",
      "Returns Hugging Face model embedding function.\n",
      "Loaded existing Vector Store from Milvus database\n",
      "Finished operations.\n"
     ]
    }
   ],
   "source": [
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "    It connects to a local Milvus database specified by the URI. If a collection named \"research_paper_chatbot\" already exists,\n",
    "    it loads the existing vector store; otherwise, it creates a new vector store and drops any existing one.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    print(\"Directory created for vector store if it did not exist\")\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "    print(\"Connected to the Milvus database\")\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if utility.has_collection(\"research_paper_chatbot\"):\n",
    "        print(\"Collection already exists. Loading existing Vector Store.\")\n",
    "        vector_store = Milvus(\n",
    "            collection_name=\"research_paper_chatbot\",\n",
    "            embedding_function=get_embedding_function(),\n",
    "            connection_args={\"uri\": uri}\n",
    "        )\n",
    "        print(\"Existing Vector Store Loaded\")\n",
    "    else:\n",
    "        vector_store = Milvus.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"research_paper_chatbot\",\n",
    "            connection_args={\"uri\": uri},\n",
    "            drop_old=True,\n",
    "        )\n",
    "        print(\"New Vector Store Created with provided documents\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def load_exisiting_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    vector_store = Milvus(\n",
    "        collection_name=\"research_paper_chatbot\",\n",
    "        embedding_function=get_embedding_function(),\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "    print(\"Loaded existing Vector Store from Milvus database\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load documents from the web\n",
    "    print(\"Loading documents from the web...\")\n",
    "    documents = load_documents_from_web()  # Load documents\n",
    "    print(f\"Loaded {len(documents)} documents from the web.\")\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    docs = split_documents(documents)  # Ensure that docs is a list of documents\n",
    "    print(f\"Split into {len(docs)} chunks.\")\n",
    "\n",
    "    # Get the embedding function\n",
    "    print(\"Getting embedding function...\")\n",
    "    embeddings = get_embedding_function()\n",
    "\n",
    "    # Define the URI for the Milvus database\n",
    "    uri = MILVUS_URI  \n",
    "\n",
    "    # Call the functions to see print statements\n",
    "    print(\"Creating vector store...\")\n",
    "    vector_store = create_vector_store(docs, embeddings, uri)\n",
    "\n",
    "    print(\"Loading existing vector store...\")\n",
    "    loaded_vector_store = load_exisiting_db(uri)\n",
    "\n",
    "    print(\"Finished operations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
