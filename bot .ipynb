{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56e1b3d0-4d24-4f1e-b57f-57f3fcb7cf1b",
   "metadata": {},
   "source": [
    "# Documentation for bot.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae825ba9-ffbd-4730-994b-9e49e2b6ae09",
   "metadata": {},
   "source": [
    "### Import libaries and setup environment variables\n",
    "\n",
    "- Imports essential modules for functionality\n",
    "  \n",
    "- Loads environment variables from a `.env` file\n",
    "\n",
    "- Sets MISTRAL API key from environment variables\n",
    "\n",
    "- Defines the URI for the Milvus database\n",
    "\n",
    "- Specifies model name for sentence embeddings\n",
    "\n",
    "- Prints initialization summary message to console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a37046ed-6dcf-4f02-97e6-c40cfacb85c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializes environment, loads documents, sets embeddings.\n"
     ]
    }
   ],
   "source": [
    "import os  # Imports the os module for interacting with the operating system\n",
    "from dotenv import load_dotenv  # Imports load_dotenv to manage environment variables from a .env file\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain  # Imports function for combining document chains\n",
    "from langchain.schema import Document  # Imports Document class for document representation\n",
    "from langchain_core.prompts import PromptTemplate  # Imports PromptTemplate for structured prompts\n",
    "#from langchain_mistralai import MistralAIEmbeddings  # (Commented out) Placeholder for MistralAI embeddings integration\n",
    "from langchain_mistralai.chat_models import ChatMistralAI  # Imports ChatMistralAI for chat model integration\n",
    "#from langchain_cohere import ChatCohere  # (Commented out) Placeholder for Cohere chat model integration\n",
    "from langchain_milvus import Milvus  # Imports Milvus for vector database integration\n",
    "from langchain_community.document_loaders import WebBaseLoader, RecursiveUrlLoader  # Imports document loaders for web content\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter  # Imports text splitter for managing document sizes\n",
    "from langchain.chains import create_retrieval_chain  # Imports function to create a retrieval chain for documents\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Imports Hugging Face embeddings integration\n",
    "from pymilvus import connections, utility  # Imports connections and utility functions for Milvus database\n",
    "from requests.exceptions import HTTPError  # Imports HTTPError for handling HTTP exceptions\n",
    "from httpx import HTTPStatusError  # Imports HTTPStatusError for handling HTTP status exceptions\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()  # Loads environment variables from a .env file into the environment\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")  # Retrieves MISTRAL_API_KEY from environment variables\n",
    "\n",
    "# Set USER_AGENT environment variable if not already set\n",
    "if not os.getenv(\"USER_AGENT\"):  # Checks if USER_AGENT is not set\n",
    "    os.environ[\"USER_AGENT\"] = \"my_custom_user_agent\"  # Sets a custom user agent string for HTTP requests\n",
    "\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  # Specifies the URI for the Milvus vector database\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Sets the model name for sentence embeddings\n",
    "CORPUS_SOURCE = 'https://dl.acm.org/doi/proceedings/10.1145/3597503'  # Sets the source URL for the document corpus\n",
    "\n",
    "print(\"Initializes environment, loads documents, sets embeddings.\")  # Prints a message indicating initialization steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed25d8f9-f26a-4641-aaa0-9c97e0194fd6",
   "metadata": {},
   "source": [
    "### Creating Hugging Face Embedding Function\n",
    "\n",
    "- Imports HuggingFaceEmbeddings for model embedding functionality\n",
    "  \n",
    "- Defines model name for sentence embeddings\n",
    "\n",
    "- Creates and returns embedding function for specified model\n",
    "\n",
    "- Prints confirmation of embedding function creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2a3f8267-7fce-4155-a0de-a9c38014adff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Returns Hugging Face model embedding function.\n",
      "Embedding function created with model: sentence-transformers/all-MiniLM-L6-v2\n"
     ]
    }
   ],
   "source": [
    "import os  # Imports the os module for interacting with the operating system\n",
    "os.environ['TQDM_DISABLE'] = '1'  # Suppress tqdm warnings by setting environment variable\n",
    "\n",
    "from langchain_huggingface import HuggingFaceEmbeddings  # Imports HuggingFaceEmbeddings for embedding functionality\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Defines the model name for embeddings\n",
    "\n",
    "def get_embedding_function():\n",
    "    \"\"\"\n",
    "    Returns embedding function for the model.\n",
    "\n",
    "    Returns:\n",
    "        embedding function\n",
    "    \"\"\"\n",
    "    embedding_function = HuggingFaceEmbeddings(model_name=MODEL_NAME)  # Creates embedding function using specified model\n",
    "    \n",
    "    print(\"Returns Hugging Face model embedding function.\")  # Print statement to confirm the embedding function is created\n",
    "    \n",
    "    return embedding_function  # Returns the created embedding function\n",
    "\n",
    "# Call the function to trigger the print statement and get the embedding function\n",
    "embedding_function = get_embedding_function()  # Calls the function and stores the resulting embedding function\n",
    "\n",
    "print(f\"Embedding function created with model: {MODEL_NAME}\")  # Prints confirmation of the embedding function creation along with the model name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346e6ef4-cf3e-4b46-807d-f168f750a83f",
   "metadata": {},
   "source": [
    "### Query response generation\n",
    "\n",
    "- Initializes ChatMistralAI model for query processing.\n",
    "\n",
    "- Defines functions for prompt and vector store management.\n",
    "\n",
    "- Creates document and retrieval chains for responses.\n",
    "\n",
    "- Generates answers based on user queries and sources.\n",
    "\n",
    "- Handles HTTP errors during query processing.\n",
    "\n",
    "- Returns generated answers with associated source links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b9fa6c9-9fd5-412c-90df-9140cdbd942c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded\n",
      "Document Chain Created\n",
      "Retrieval Chain Created\n",
      "Response Generated\n",
      "Initializes components, retrieves documents, generates response.\n",
      "Generated response\n",
      "\n",
      "Sources: [Source 1](https://example.com/research_paper1), [Source 2](https://example.com/research_paper2)\n"
     ]
    }
   ],
   "source": [
    "from langchain_mistralai.chat_models import ChatMistralAI  # Imports the ChatMistralAI model for conversation generation\n",
    "from httpx import HTTPStatusError  # Imports HTTPStatusError for handling HTTP-related exceptions\n",
    "\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # Defines the model name for embeddings\n",
    "MILVUS_URI = \"./milvus/milvus_vector.db\"  # Specifies the URI for the Milvus vector database\n",
    "\n",
    "def create_prompt():\n",
    "    # Placeholder function for creating a prompt\n",
    "    return \"Provide a detailed summary of the latest research on: {input}\"  # Returns a prompt template for querying\n",
    "\n",
    "def load_exisiting_db(uri):\n",
    "    # Placeholder function for loading the vector store\n",
    "    class VectorStore:  # Defines a local class for the vector store\n",
    "        def as_retriever(self):  # Method to return itself as a retriever\n",
    "            return self  # Returns the instance of VectorStore\n",
    "    return VectorStore()  # Returns an instance of VectorStore\n",
    "\n",
    "def create_stuff_documents_chain(model, prompt):\n",
    "    # Placeholder function for creating a document chain\n",
    "    return \"Document chain here.\"  # Returns a string indicating where the document chain would be\n",
    "\n",
    "def create_retrieval_chain(retriever, document_chain):\n",
    "    # Placeholder function for creating a retrieval chain\n",
    "    return lambda x: {  # Returns a lambda function to generate a response\n",
    "        \"context\": [  # Contextual information with metadata for sources\n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper1\"}}, \n",
    "            {\"metadata\": {\"source\": \"https://example.com/research_paper2\"}}\n",
    "        ], \n",
    "        \"answer\": \"Generated response\"  # Sample generated answer\n",
    "    }\n",
    "\n",
    "def query_rag(query):\n",
    "    \"\"\"\n",
    "    Entry point for the RAG model to generate an answer to a given query.\n",
    "\n",
    "    This function initializes the RAG model, sets up the necessary components such as the prompt template, vector store, \n",
    "    retriever, document chain, and retrieval chain, and then generates a response to the provided query.\n",
    "\n",
    "    Args:\n",
    "        query (str): The query string for which an answer is to be generated.\n",
    "    \n",
    "    Returns:\n",
    "        str: The answer to the query\n",
    "    \"\"\"\n",
    "    # Define the model\n",
    "    model = ChatMistralAI(model='open-mistral-7b')  # Initializes the ChatMistralAI model\n",
    "    print(\"Model Loaded\")  # Print statement confirming the model has loaded\n",
    "\n",
    "    prompt = create_prompt()  # Calls the function to create the prompt\n",
    "\n",
    "    # Load the vector store and create the retriever\n",
    "    vector_store = load_exisiting_db(uri=MILVUS_URI)  # Loads the existing vector store\n",
    "    retriever = vector_store.as_retriever()  # Calls method to get the retriever from the vector store\n",
    "    \n",
    "    try:\n",
    "        document_chain = create_stuff_documents_chain(model, prompt)  # Creates a document chain\n",
    "        print(\"Document Chain Created\")  # Print confirmation for document chain creation\n",
    "\n",
    "        retrieval_chain = create_retrieval_chain(retriever, document_chain)  # Creates a retrieval chain\n",
    "        print(\"Retrieval Chain Created\")  # Print confirmation for retrieval chain creation\n",
    "    \n",
    "        # Generate a response to the query\n",
    "        response = retrieval_chain({\"input\": f\"{query}\"})  # Calls the retrieval chain with the query input\n",
    "    except HTTPStatusError as e:  # Catches HTTP errors\n",
    "        print(f\"HTTPStatusError: {e}\")  # Print the error message\n",
    "        if e.response.status_code == 429:  # Checks for rate limit error\n",
    "            return \"I am currently experiencing high traffic. Please try again later.\", []  # Returns message for high traffic\n",
    "        return f\"HTTPStatusError: {e}\", []  # Returns error message for other HTTP errors\n",
    "    \n",
    "    # Logic to add sources to the response\n",
    "    max_relevant_sources = 4  # Defines maximum number of sources to include\n",
    "    all_sources = \"\"  # Initialize string to hold all source links\n",
    "    sources = []  # Initialize list to hold unique sources\n",
    "    count = 1  # Initialize counter for source numbering\n",
    "\n",
    "    for i in range(max_relevant_sources):  # Loop over the number of maximum relevant sources\n",
    "        try:\n",
    "            source = response[\"context\"][i][\"metadata\"][\"source\"]  # Retrieves source from response context\n",
    "            # Check if the source is already added to the list\n",
    "            if source not in sources:  # If source is unique\n",
    "                sources.append(source)  # Add source to the list\n",
    "                all_sources += f\"[Source {count}]({source}), \"  # Append formatted source link to all_sources\n",
    "                count += 1  # Increment the source counter\n",
    "        except IndexError:  # Handle case where there are no more sources\n",
    "            break  # Exit the loop if no more sources are available\n",
    "            \n",
    "    all_sources = all_sources[:-2]  # Remove the last comma and space from all_sources\n",
    "    response[\"answer\"] += f\"\\n\\nSources: {all_sources}\"  # Append all sources to the answer\n",
    "    print(\"Response Generated\")  # Print confirmation of response generation\n",
    "\n",
    "    print(\"Initializes components, retrieves documents, generates response.\")  # Summary of what the function does\n",
    "\n",
    "    return response[\"answer\"], sources  # Return the generated answer and list of sources\n",
    "\n",
    "# Example usage\n",
    "query = \"Latest research on machine learning in healthcare\"  # Defines an example query\n",
    "answer, sources = query_rag(query)  # Calls the query_rag function with the example query\n",
    "print(answer)  # Prints the generated answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2220aed-32ea-4f03-9f56-9b36feb3f511",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "- `create_vector_store` function is designed to manage a vector store using a Milvus database.\n",
    "\n",
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "    It connects to a local Milvus database specified by the URI. If a collection named \"research_paper_chatbot\" already exists,\n",
    "    it loads the existing vector store; otherwise, it creates a new vector store and drops any existing one.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "\n",
    "- Create the directory if it does not exist\n",
    "    `head = os.path.split(uri)` : Split the URI to get the directory path\n",
    "\n",
    "   `os.makedirs(head[0], exist_ok=True)`  : Create the directory if it doesn't exist\n",
    "\n",
    "    `print(\"Directory created for vector store if it did not exist\")` : Print confirmation of directory creation\n",
    "\n",
    "- Connect to the Milvus database\n",
    "\n",
    "    `connections.connect(\"default\", uri=uri)` : Establish a connection to the Milvus database\n",
    "\n",
    "    `print(\"Connected to the Milvus database\")` : Print confirmation of database connection\n",
    "\n",
    "- Check if the collection already exists\n",
    "\n",
    "- `if utility.has_collection(\"research_paper_chatbot\"):` - Check for existing collection\n",
    "\n",
    "- `print(\"Collection already exists. Loading existing Vector Store.\")` : Print collection status\n",
    "\n",
    "- `vector_store = Milvus` : Load existing vector store\n",
    "\n",
    "-  Get the embedding function\n",
    "\n",
    "  `collection_name=\"research_paper_chatbot\",\n",
    "            embedding_function=get_embedding_function()`\n",
    "            \n",
    "- `connection_args={\"uri\": uri}` - Connection parameters\n",
    "\n",
    "     `print(\"Existing Vector Store Loaded\")` : Print confirmation of loading existing store\n",
    "- else:\n",
    "  `vector_store = Milvus.from_documents` : Create a new vector store from documents\n",
    "\n",
    "  `documents=docs,` : Documents to store\n",
    "  \n",
    "   `embedding=embeddings,`  : Embedding function for documents\n",
    "\n",
    "   `collection_name=\"research_paper_chatbot\",` : Name of the collection\n",
    "\n",
    "   `connection_args={\"uri\": uri},` : Connection parameters\n",
    "  \n",
    "- `drop_old=True,` : Drop old collection if exists\n",
    "        )\n",
    "- `print(\"New Vector Store Created with provided documents\")` - Print confirmation of new store creation\n",
    "\n",
    "- `return vector_store` : Return the created or loaded vector store\n",
    "\n",
    "def load_exisiting_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "\n",
    "- `vector_store = Milvus(`  : Load the vector store\n",
    "\n",
    "- `collection_name=\"research_paper_chatbot\",` : Name of the collection\n",
    "\n",
    "- `embedding_function=get_embedding_function(),` : Get the embedding function\n",
    "\n",
    "- `connection_args={\"uri\": uri},` Connection parameters\n",
    "    )\n",
    "    \n",
    "- `print(\"Loaded existing Vector Store from Milvus database\")` : Print confirmation of store loading\n",
    "\n",
    "- `return vector_store` : Return the loaded vector store\n",
    "\n",
    "`if __name__ == '__main__':`:  Load documents from the web\n",
    "    \n",
    "- `print(\"Loading documents from the web...\")` : Print message before loading documents\n",
    "\n",
    "- `documents = load_documents_from_web()` : Load documents from the web\n",
    "\n",
    "- `print(f\"Loaded {len(documents)} documents from the web.\")` : Print number of documents loaded\n",
    "\n",
    "- Split the documents into chunks\n",
    "- `print(\"Splitting documents into chunks...\")` : Print message before splitting documents\n",
    "\n",
    "- `docs = split_documents(documents)` : Split loaded documents into chunk\n",
    "  \n",
    "- `print(f\"Split into {len(docs)} chunks.\")` : Print number of chunks created\n",
    "\n",
    "- Get the embedding function\n",
    "    `print(\"Getting embedding function...\")`\n",
    "      \n",
    "- `embeddings = get_embedding_function()`  : Retrieve the embedding function\n",
    "\n",
    "- Define the URI for the Milvus database Assign the URI for the Milvus database\n",
    "\n",
    "    `uri = MILVUS_URI`\n",
    "  \n",
    "- Call the functions to see print statements\n",
    "\n",
    "- `print(\"Creating vector store...\")` : Print message before creating vector store\n",
    "\n",
    "- `vector_store = create_vector_store(docs, embeddings, uri)` : Create the vector store\n",
    "\n",
    "- `print(\"Loading existing vector store...\")`  : Print message before loading existing store\n",
    "\n",
    "    `loaded_vector_store = load_exisiting_db(uri)`  : Load the existing vector store\n",
    "\n",
    "- `print(\"Finished operations.\")` : Print message indicating completion of operations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7fcdf03a-ad46-4b5b-9a6b-b27e970939b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading documents from the web...\n",
      "Loaded 1 documents from the web.\n",
      "Splitting documents into chunks...\n",
      "Split into 6 chunks.\n",
      "Getting embedding function...\n",
      "Returns Hugging Face model embedding function.\n",
      "Creating vector store...\n",
      "Directory created for vector store if it did not exist\n",
      "Connected to the Milvus database\n",
      "Collection already exists. Loading existing Vector Store.\n",
      "Returns Hugging Face model embedding function.\n",
      "Existing Vector Store Loaded\n",
      "Loading existing vector store...\n",
      "Returns Hugging Face model embedding function.\n",
      "Loaded existing Vector Store from Milvus database\n",
      "Finished operations.\n"
     ]
    }
   ],
   "source": [
    "def create_vector_store(docs, embeddings, uri):\n",
    "    \"\"\"\n",
    "    This function initializes a vector store using the provided documents and embeddings.\n",
    "    It connects to a local Milvus database specified by the URI. If a collection named \"research_paper_chatbot\" already exists,\n",
    "    it loads the existing vector store; otherwise, it creates a new vector store and drops any existing one.\n",
    "\n",
    "    Args:\n",
    "        docs (list): A list of documents to be stored in the vector store.\n",
    "        embeddings : A function or model that generates embeddings for the documents.\n",
    "        uri (str): Path to the local milvus db\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    # Create the directory if it does not exist\n",
    "    head = os.path.split(uri)\n",
    "    os.makedirs(head[0], exist_ok=True)\n",
    "    print(\"Directory created for vector store if it did not exist\")\n",
    "\n",
    "    # Connect to the Milvus database\n",
    "    connections.connect(\"default\", uri=uri)\n",
    "    print(\"Connected to the Milvus database\")\n",
    "\n",
    "    # Check if the collection already exists\n",
    "    if utility.has_collection(\"research_paper_chatbot\"):\n",
    "        print(\"Collection already exists. Loading existing Vector Store.\")\n",
    "        vector_store = Milvus(\n",
    "            collection_name=\"research_paper_chatbot\",\n",
    "            embedding_function=get_embedding_function(),\n",
    "            connection_args={\"uri\": uri}\n",
    "        )\n",
    "        print(\"Existing Vector Store Loaded\")\n",
    "    else:\n",
    "        vector_store = Milvus.from_documents(\n",
    "            documents=docs,\n",
    "            embedding=embeddings,\n",
    "            collection_name=\"research_paper_chatbot\",\n",
    "            connection_args={\"uri\": uri},\n",
    "            drop_old=True,\n",
    "        )\n",
    "        print(\"New Vector Store Created with provided documents\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "def load_exisiting_db(uri=MILVUS_URI):\n",
    "    \"\"\"\n",
    "    Load an existing vector store from the local Milvus database specified by the URI.\n",
    "\n",
    "    Args:\n",
    "        uri (str, optional): Path to the local milvus db. Defaults to MILVUS_URI.\n",
    "\n",
    "    Returns:\n",
    "        vector_store: The vector store created\n",
    "    \"\"\"\n",
    "    vector_store = Milvus(\n",
    "        collection_name=\"research_paper_chatbot\",\n",
    "        embedding_function=get_embedding_function(),\n",
    "        connection_args={\"uri\": uri},\n",
    "    )\n",
    "    print(\"Loaded existing Vector Store from Milvus database\")\n",
    "    return vector_store\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Load documents from the web\n",
    "    print(\"Loading documents from the web...\")\n",
    "    documents = load_documents_from_web()  # Load documents\n",
    "    print(f\"Loaded {len(documents)} documents from the web.\")\n",
    "\n",
    "    # Split the documents into chunks\n",
    "    print(\"Splitting documents into chunks...\")\n",
    "    docs = split_documents(documents)  # Ensure that docs is a list of documents\n",
    "    print(f\"Split into {len(docs)} chunks.\")\n",
    "\n",
    "    # Get the embedding function\n",
    "    print(\"Getting embedding function...\")\n",
    "    embeddings = get_embedding_function()\n",
    "\n",
    "    # Define the URI for the Milvus database\n",
    "    uri = MILVUS_URI  \n",
    "\n",
    "    # Call the functions to see print statements\n",
    "    print(\"Creating vector store...\")\n",
    "    vector_store = create_vector_store(docs, embeddings, uri)\n",
    "\n",
    "    print(\"Loading existing vector store...\")\n",
    "    loaded_vector_store = load_exisiting_db(uri)\n",
    "\n",
    "    print(\"Finished operations.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
